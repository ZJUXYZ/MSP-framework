{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fno import FNO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data_process import DataProcess, split_data, delete_row, mirror_fill\n",
    "from index_cal import mse_c, mae_c, std_c, r2_c, iqr_c\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNO ti+1 processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable = pd.read_csv('stablechange_data.csv', header = None).to_numpy()\n",
    "random = pd.read_csv('randomchange_data.csv', header = None).to_numpy()\n",
    "\n",
    "stabledl = pd.read_csv('stablechange_dl.csv', header = None).to_numpy()\n",
    "randomdl = pd.read_csv('randomchange_dl.csv', header = None).to_numpy()\n",
    "\n",
    "stablekapa = pd.read_csv('stablechange_kapa.csv', header = None).to_numpy()\n",
    "randomkapa = pd.read_csv('randomchange_kapa.csv', header = None).to_numpy()\n",
    "\n",
    "stabletau = pd.read_csv('stablechange_tau.csv', header = None).to_numpy()\n",
    "randomtau = pd.read_csv('randomchange_tau.csv', header = None).to_numpy()\n",
    "\n",
    "stablekapaidea = pd.read_csv('stablechange_kapa_idea.csv', header = None).to_numpy()\n",
    "randomkapaidea = pd.read_csv('randomchange_kapa_idea.csv', header = None).to_numpy()\n",
    "\n",
    "stabletauidea = pd.read_csv('stablechange_tau_idea.csv', header = None).to_numpy()\n",
    "randomtauidea = pd.read_csv('randomchange_tau_idea.csv', header = None).to_numpy()\n",
    "\n",
    "stablealpha = pd.read_csv('stablechange_alpha.csv', header = None).to_numpy()\n",
    "randomalpha = pd.read_csv('randomchange_alpha.csv', header = None).to_numpy()\n",
    "\n",
    "stableU = pd.read_csv('stablechange_U.csv', header = None).to_numpy()\n",
    "randomU = pd.read_csv('randomchange_U.csv', header = None).to_numpy()\n",
    "\n",
    "stablew = pd.read_csv('stablechange_w.csv', header = None).to_numpy()\n",
    "randomw = pd.read_csv('randomchange_w.csv', header = None).to_numpy()\n",
    "\n",
    "stable = torch.from_numpy(stable).float().to(device)\n",
    "random = torch.from_numpy(random).float().to(device)\n",
    "\n",
    "stabledl = torch.from_numpy(stabledl).float().to(device)\n",
    "randomdl = torch.from_numpy(randomdl).float().to(device)\n",
    "\n",
    "stablekapa = torch.from_numpy(stablekapa).float().to(device)\n",
    "randomkapa = torch.from_numpy(randomkapa).float().to(device)\n",
    "\n",
    "stabletau = torch.from_numpy(stabletau).float().to(device)\n",
    "randomtau = torch.from_numpy(randomtau).float().to(device)\n",
    "\n",
    "stablekapaidea = torch.from_numpy(stablekapaidea).float().to(device)\n",
    "randomkapaidea = torch.from_numpy(randomkapaidea).float().to(device)\n",
    "\n",
    "stabletauidea = torch.from_numpy(stabletauidea).float().to(device)\n",
    "randomtauidea = torch.from_numpy(randomtauidea).float().to(device)\n",
    "\n",
    "stablealpha = torch.from_numpy(stablealpha).float().to(device)\n",
    "randomalpha = torch.from_numpy(randomalpha).float().to(device)\n",
    "\n",
    "stableU = torch.from_numpy(stableU).float().to(device)\n",
    "randomU = torch.from_numpy(randomU).float().to(device)\n",
    "\n",
    "stablew = torch.from_numpy(stablew).float().to(device)\n",
    "randomw = torch.from_numpy(randomw).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = [0, 1, 2, 3, 4, 5, 12]\n",
    "index2 = [0, 1, 2, 3, 4, 5, 23]\n",
    "\n",
    "feature1 = stable[:, index1]\n",
    "feature2 = random[:, index2]\n",
    "\n",
    "feature_normalize11 = feature1.repeat_interleave(18, dim = 0)\n",
    "feature_normalize22 = feature2.repeat_interleave(8, dim = 0)\n",
    "\n",
    "stabledl = stabledl[:, 1:-1].reshape(-1, 1)\n",
    "randomdl = randomdl[:, 1:-1].reshape(-1, 1)\n",
    "\n",
    "stabledl = stabledl\n",
    "randomdl = randomdl\n",
    "\n",
    "feature_normalize11 = torch.cat((feature_normalize11, stabledl), dim = 1) # [allsize, 8]\n",
    "feature_normalize22 = torch.cat((feature_normalize22, randomdl), dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local coefficient\n",
    "num1_s = torch.round(stable[:, 13:14]).int()\n",
    "num2_s = torch.round(stable[:, 14:15]).int()\n",
    "local_k1 = torch.ones((stable.shape[0], 1200)).to(device)\n",
    "local_k2 = torch.ones((stable.shape[0], 1200)).to(device)\n",
    "\n",
    "for i in range(stable.shape[0]):\n",
    "    local_k1[i, num1_s[i] * 60:num2_s[i] * 60] = local_k1[i, num1_s[i] * 60:num2_s[i] * 60] * stable[i, 15:16]\n",
    "    local_k1[i, num2_s[i] * 60:] = local_k1[i, num2_s[i] * 60:] * stable[i, 16:17]\n",
    "\n",
    "    local_k2[i, num1_s[i] * 60:num2_s[i] * 60] = local_k2[i, num1_s[i] * 60:num2_s[i] * 60] * stable[i, 17:18]\n",
    "    local_k2[i, num2_s[i] * 60:] = local_k2[i, num2_s[i] * 60:] * stable[i, 18:19]\n",
    "\n",
    "\n",
    "num1_r = torch.round(random[:, 24:25]).int()\n",
    "local_k1r = torch.ones((random.shape[0], 600)).to(device)\n",
    "local_k2r = torch.ones((random.shape[0], 600)).to(device)\n",
    "\n",
    "for i in range(random.shape[0]):\n",
    "    local_k1r[i, num1_r[i] * 60:] = local_k1r[i, num1_r[i] * 60:] * random[i, 25:26]\n",
    "    local_k2r[i, num1_r[i] * 60:] = local_k2r[i, num1_r[i] * 60:] * random[i, 26:27]\n",
    "\n",
    "local_k1 = local_k1.repeat_interleave(18, dim = 0)\n",
    "local_k2 = local_k2.repeat_interleave(18, dim = 0)\n",
    "local_k1r = local_k1r.repeat_interleave(8, dim = 0)\n",
    "local_k2r = local_k2r.repeat_interleave(8, dim = 0)\n",
    "\n",
    "local_k1 = torch.cat((local_k1, local_k1, local_k1), dim = 1)\n",
    "local_k2 = torch.cat((local_k2, local_k2, local_k2), dim = 1)\n",
    "local_k1r = torch.cat((local_k1r, local_k1r, local_k1r), dim = 1)\n",
    "local_k2r = torch.cat((local_k2r, local_k2r, local_k2r), dim = 1)\n",
    "\n",
    "local_k1[stablekapa == 0] = 0\n",
    "local_k2[stablekapa == 0] = 0\n",
    "local_k1r[randomkapa == 0] = 0\n",
    "local_k2r[randomkapa == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_num = 10\n",
    "feature_normalize11, valid_feature_normalize11 = split_data(feature_normalize11, -18 * valid_num)\n",
    "feature_normalize22, valid_feature_normalize22 = split_data(feature_normalize22, -8 * valid_num)\n",
    "stablekapa, valid_stablekapa = split_data(stablekapa, -18 * valid_num)\n",
    "randomkapa, valid_randomkapa = split_data(randomkapa, -8 * valid_num)\n",
    "stabletau, valid_stabletau = split_data(stabletau, -18 * valid_num)\n",
    "randomtau, valid_randomtau = split_data(randomtau, -8 * valid_num)\n",
    "stablekapaidea, valid_stablekapaidea = split_data(stablekapaidea, -18 * valid_num)\n",
    "randomkapaidea, valid_randomkapaidea = split_data(randomkapaidea, -8 * valid_num)\n",
    "stabletauidea, valid_stabletauidea = split_data(stabletauidea, -18 * valid_num)\n",
    "randomtauidea, valid_randomtauidea = split_data(randomtauidea, -8 * valid_num)\n",
    "stablealpha, valid_stablealpha = split_data(stablealpha, -18 * valid_num)\n",
    "randomalpha, valid_randomalpha = split_data(randomalpha, -8 * valid_num)\n",
    "stableU, valid_stableU = split_data(stableU, -18 * valid_num)\n",
    "randomU, valid_randomU = split_data(randomU, -8 * valid_num)\n",
    "stablew, valid_stablew = split_data(stablew, -18 * valid_num)\n",
    "randomw, valid_randomw = split_data(randomw, -8 * valid_num)\n",
    "\n",
    "local_k1, valid_local_k1 = split_data(local_k1, -18 * valid_num)\n",
    "local_k2, valid_local_k2 = split_data(local_k2, -18 * valid_num)\n",
    "local_k1r, valid_local_k1r = split_data(local_k1r, -8 * valid_num)\n",
    "local_k2r, valid_local_k2r = split_data(local_k2r, -8 * valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_normalize11, stablekapa, stabletau, stablekapaidea, stabletauidea, stablealpha, stableU, stablew, local_k1, local_k2 = delete_row(0.01333, feature_normalize11, stablekapa, stabletau, stablekapaidea, stabletauidea, stablealpha, stableU, stablew, local_k1, local_k2)\n",
    "feature_normalize22, randomkapa, randomtau, randomkapaidea, randomtauidea, randomalpha, randomU, randomw, local_k1r, local_k2r = delete_row(0.01333, feature_normalize22, randomkapa, randomtau, randomkapaidea, randomtauidea, randomalpha, randomU, randomw, local_k1r, local_k2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, test_feature, train_mask, test_mask, train_process1, test_process1, train_idea1, test_idea1, train_true1, test_true1, train_know, test_know, train_mask1, test_mask1, train_process2, test_process2, train_idea2, test_idea2, train_true2, test_true2, train_mask2, test_mask2, train_k1, test_k1, train_k2, test_k2 = DataProcess(feature_normalize11, stablekapa, stabletau, stablekapaidea, stabletauidea, stablealpha, stableU, stablew, feature_normalize22, randomkapa, randomtau, randomkapaidea, randomtauidea, randomalpha, randomU, randomw, local_k1, local_k2, local_k1r, local_k2r, seed = 1, num_sample1 = 1800, num_sample2 = 1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mean = train_feature.mean(dim = 0)\n",
    "feature_std = train_feature.std(dim = 0) + 1e-6\n",
    "\n",
    "process_mean = train_process1.sum(dim = (0, 1)) / train_mask1.unsqueeze(2).sum(dim = (0, 1))\n",
    "process_std = ((train_process1 - process_mean) * train_mask1.unsqueeze(2)) ** 2\n",
    "process_std1 = torch.sqrt(process_std.sum(dim = (0, 1)) / train_mask1.unsqueeze(2).sum(dim = (0, 1)) + 1e-6)\n",
    "\n",
    "idea_mean = train_idea1.sum(dim = (0, 1)) / train_mask1.unsqueeze(2).sum(dim = (0, 1))\n",
    "idea_std = ((train_idea1 - idea_mean) * train_mask1.unsqueeze(2)) ** 2\n",
    "idea_std1 = torch.sqrt(idea_std.sum(dim = (0, 1)) / train_mask1.unsqueeze(2).sum(dim = (0, 1)) + 1e-6)\n",
    "\n",
    "train_feature = (train_feature - feature_mean) / feature_std\n",
    "test_feature = (test_feature - feature_mean) / feature_std\n",
    "\n",
    "train_process1 = (train_process1 - process_mean) / process_std1 * train_mask1.unsqueeze(2)\n",
    "test_process1 = (test_process1 - process_mean) / process_std1 * test_mask1.unsqueeze(2)\n",
    "\n",
    "train_idea1 = (train_idea1 - idea_mean) / idea_std1 * train_mask1.unsqueeze(2)\n",
    "test_idea1 = (test_idea1 - idea_mean) / idea_std1 * test_mask1.unsqueeze(2)\n",
    "\n",
    "train_know[:, :, 0] = train_know[:, :, 0] * 100\n",
    "test_know[:, :, 0] = test_know[:, :, 0] * 100\n",
    "train_know[:, :, 1] = train_know[:, :, 1] * 300\n",
    "test_know[:, :, 1] = test_know[:, :, 1] * 300\n",
    "\n",
    "train_true1[:, :, 0] = train_true1[:, :, 0] * 100\n",
    "test_true1[:, :, 0] = test_true1[:, :, 0] * 100\n",
    "train_true1[:, :, 1] = train_true1[:, :, 1] * 300\n",
    "test_true1[:, :, 1] = test_true1[:, :, 1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_know = mirror_fill(train_know)\n",
    "test_know = mirror_fill(test_know)\n",
    "train_true1 = mirror_fill(train_true1)\n",
    "test_true1 = mirror_fill(test_true1)\n",
    "train_process1 = mirror_fill(train_process1)\n",
    "test_process1 = mirror_fill(test_process1)\n",
    "train_idea1 = mirror_fill(train_idea1)\n",
    "test_idea1 = mirror_fill(test_idea1)\n",
    "train_k1 = mirror_fill(train_k1)\n",
    "test_k1 = mirror_fill(test_k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.linspace(0, 1, 1200).reshape(1200, 1)\n",
    "t_s = t1.to(device)\n",
    "train_knows = torch.cat((train_know, t_s.repeat(train_feature.shape[0], 1, 1)), dim = 2)\n",
    "test_knows = torch.cat((test_know, t_s.repeat(test_feature.shape[0], 1, 1)), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_feature.unsqueeze(2)\n",
    "x1 = torch.cat((train_process1, train_k1, train_knows), dim = 2)\n",
    "x_mask = train_mask.unsqueeze(2) * train_mask1.unsqueeze(2)\n",
    "x1 = x1.permute(0, 2, 1)\n",
    "\n",
    "x_train = (x, x1, x_mask)\n",
    "y_train = train_true1\n",
    "\n",
    "x = test_feature.unsqueeze(2)\n",
    "x2 = torch.cat((test_process1, test_k1, test_knows), dim = 2)\n",
    "x_mask2 = test_mask.unsqueeze(2) * test_mask1.unsqueeze(2)\n",
    "x2 = x2.permute(0, 2, 1)\n",
    "\n",
    "x_test = (x, x2, x_mask2)\n",
    "y_test = test_true1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature, input, mask = self.data[0], self.data[1], self.data[2]\n",
    "        label1 = self.label\n",
    "        \n",
    "        feature1, input1, masks = feature[idx], input[idx], mask[idx]\n",
    "        label11 = label1[idx]\n",
    "\n",
    "        x = (feature1, input1, masks)\n",
    "        y = (label11)        \n",
    "\n",
    "        return x, y\n",
    "    \n",
    "train_loader = DataLoader(CustomDataset(x_train, y_train), batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(CustomDataset(x_test, y_test), batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNO(n_modes = [64], in_channels = 8, out_channels = 1, hidden_channels = 64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1.2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 500, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1200\n",
    "# min_loss = 7e-5\n",
    "# min_loss2 = 9e-5\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "lossf = nn.MSELoss(reduction = 'none')\n",
    "for i in range(steps):\n",
    "    model.train()\n",
    "    for data, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data[0], data[1])\n",
    "        label = label[:, :, 0:1]\n",
    "        loss1 = lossf(out * data[2], label * data[2]).sum() / data[2].sum()\n",
    "        # loss1 = lossf(out * data[2], label * data[2]).sum() / data[2].sum() / 2\n",
    "        diff1 = (out[:, :-2] - 2 * out[:, 1:-1] + out[:, 2:]) * data[2][:, 2:] / 5\n",
    "        loss2 = (diff1 ** 2).sum() / data[2][:, 2:].sum() / 2\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "    loss_list.append(loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            out = model(data[0], data[1])\n",
    "            label = label[:, :, 0:1]\n",
    "            test_loss = lossf(out * data[2], label * data[2]).sum() / data[2].sum()\n",
    "            # test_loss = lossf(out * data[2], label * data[2]).sum() / data[2].sum() / 2\n",
    "            total_test_loss += test_loss.item() * len(data[0])\n",
    "            total_test_samples += len(data[0])\n",
    "\n",
    "        average_test_loss = total_test_loss / total_test_samples\n",
    "        test_loss_list.append(average_test_loss)\n",
    "\n",
    "    if (i + 1) % 300 == 0:\n",
    "        print('+++++++++++++', i + 1, '+++++++++++++')\n",
    "        print('loss is: ', loss1.detach().cpu().numpy())\n",
    "        print('test_loss is: ', average_test_loss)\n",
    "\n",
    "    if loss1 < min_loss and average_test_loss < min_loss2:\n",
    "        min_loss = loss1\n",
    "        min_loss2 = average_test_loss\n",
    "        torch.save(model.state_dict(), 'modelFNOtest1_ok.pth')\n",
    "        print('min_loss is: ', min_loss.detach().cpu().numpy(), 'loss is', average_test_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
